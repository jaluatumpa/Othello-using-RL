{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CBNaDyWyqmlZ",
   "metadata": {
    "id": "CBNaDyWyqmlZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "uE6DyOozqwGX",
   "metadata": {
    "id": "uE6DyOozqwGX"
   },
   "source": [
    "## Required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39bca6a1",
   "metadata": {
    "id": "39bca6a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\tumpa_anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.optimizers\n",
    "tf.keras.utils.disable_interactive_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v52QFdZSqhg8",
   "metadata": {
    "id": "v52QFdZSqhg8"
   },
   "source": [
    "## Game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968aae84",
   "metadata": {
    "id": "968aae84"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Board:\n",
    "    def __init__(self, player1, player2, n=6):\n",
    "        self.n = n\n",
    "        self.state_size = n * n\n",
    "        self.action_size = n * n\n",
    "        self.board = [[0] * n for _ in range(n)]\n",
    "        self.current_play = 1\n",
    "        self.reset()\n",
    "        self.player1 = player1\n",
    "        self.player2 = player2\n",
    "        self.done = False\n",
    "        self.directions = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n",
    "\n",
    "    def reset(self):\n",
    "        n = self.n\n",
    "        self.board = [[0] * n for _ in range(n)]\n",
    "        self.current_play = 1\n",
    "        if n % 2 == 0:\n",
    "            mid = n // 2\n",
    "            self.board[mid - 1][mid - 1] = 1\n",
    "            self.board[mid][mid] = 1\n",
    "            self.board[mid - 1][mid] = -1\n",
    "            self.board[mid][mid- 1] = -1\n",
    "\n",
    "    def get_board_state(self):\n",
    "        return self.board\n",
    "\n",
    "    def count_diff(self, color):\n",
    "        count = 0\n",
    "        for j in range(self.n):\n",
    "            for i in range(self.n):\n",
    "                if self.board[i][j] == color:\n",
    "                    count += 1\n",
    "                if self.board[i][j] == -color:\n",
    "                    count -= 1\n",
    "        return count\n",
    "\n",
    "    def get_legal_moves(self, color):\n",
    "        moves = set()\n",
    "        for j in range(self.n):\n",
    "            for i in range(self.n):\n",
    "                if self.board[i][j] == color:\n",
    "                    new_moves = self.get_moves_for_square((i, j))\n",
    "                    moves.update(new_moves)\n",
    "        return list(moves)\n",
    "\n",
    "    def has_legal_moves(self, color):\n",
    "        for j in range(self.n):\n",
    "            for i in range(self.n):\n",
    "                if self.board[i][j] == color:\n",
    "                    new_moves = self.get_moves_for_square((i, j))\n",
    "                    if len(new_moves) > 0:\n",
    "                        return True\n",
    "        return False\n",
    "\n",
    "    def get_moves_for_square(self, square):\n",
    "        (i, j) = square\n",
    "        color = self.board[i][j]\n",
    "        if color == 0:\n",
    "            return None\n",
    "        moves = []\n",
    "        for direction in self.directions:\n",
    "            move = self._discover_move(square, direction)\n",
    "            if move:\n",
    "                moves.append(move)\n",
    "        return moves\n",
    "\n",
    "    def execute_move(self, move, color):\n",
    "        flips = []\n",
    "        for direction in self.directions:\n",
    "            flip = self._get_flips(move, direction, color)\n",
    "            flips.extend(flip)\n",
    "\n",
    "        if flips:\n",
    "            for i, j in flips:\n",
    "                self.board[i][j] = color\n",
    "        else:\n",
    "            raise Exception(\"Invalid move\")\n",
    "        row, col = move\n",
    "        self.board[row][col] = color\n",
    "        self.current_play = 1 if self.current_play == 1 else -1\n",
    "\n",
    "    def _discover_move(self, origin, direction):\n",
    "        i, j = origin\n",
    "        color = self.board[i][j]\n",
    "        flips = []\n",
    "        for new_i, new_j in self._increment_move(origin, direction, self.n):\n",
    "            if self.board[new_i][new_j] == 0:\n",
    "                if flips:\n",
    "                    return (new_i, new_j)\n",
    "                else:\n",
    "                    return None\n",
    "            elif self.board[new_i][new_j] == color:\n",
    "                return None\n",
    "            elif self.board[new_i][new_j] == -color:\n",
    "                flips.append((new_i, new_j))\n",
    "        return None\n",
    "\n",
    "    def _get_flips(self, origin, direction, color):\n",
    "        flips = []\n",
    "        for i, j in self._increment_move(origin, direction, self.n):\n",
    "            if self.board[i][j] == 0:\n",
    "                return []\n",
    "            if self.board[i][j] == -color:\n",
    "                flips.append((i, j))\n",
    "            elif self.board[i][j] == color and len(flips) > 0:\n",
    "                return flips\n",
    "        return []\n",
    "\n",
    "    def _increment_move(self, move, direction, n):\n",
    "        move = tuple(move)[:2]  # Convert to tuple if it's a list\n",
    "        i, j = move\n",
    "        move = (i + direction[0], j + direction[1])\n",
    "        while 0 <= move[0] < n and 0 <= move[1] < n:\n",
    "            yield move\n",
    "            move = (move[0] + direction[0], move[1] + direction[1])\n",
    "\n",
    "    def get_reward(self):\n",
    "        player1_score = self.count_diff(1)\n",
    "        player2_score = self.count_diff(-1)\n",
    "        if player1_score > player2_score:   \n",
    "            return 1\n",
    "        elif player1_score < player2_score: \n",
    "            return -1\n",
    "        else: \n",
    "            return 0\n",
    "\n",
    "    def game_over(self):\n",
    "        return not self.has_legal_moves(1) and not self.has_legal_moves(-1)\n",
    "\n",
    "    ## Trainning(wo player play against each other)\n",
    "    def play(self, rounds=200,inner_rounds=10):\n",
    "        results = []\n",
    "        for i in range(rounds):\n",
    "            self.reset()\n",
    "            current_player = self.player1 if self.current_play == 1 else self.player2\n",
    "            if i % 100 == 0:\n",
    "                print(\"Rounds {}\".format(i))\n",
    "            if i % 10 == 0:\n",
    "                self.player1.setEps(rounds, i)\n",
    "                self.player2.setEps(rounds, i)\n",
    "            while True:\n",
    "#                 self.print_board()\n",
    "                positions = self.has_legal_moves(current_player.color)\n",
    "                if positions:\n",
    "                    p_move = current_player.choose_action(self)\n",
    "                    self.execute_move(p_move, current_player.color)\n",
    "                    current_player.addState(self.get_board_state())\n",
    "                    state_after = copy.deepcopy(self.get_board_state())\n",
    "                    current_player.addState(state_after)\n",
    "                    if self.game_over():\n",
    "                        winner = self.get_reward()\n",
    "                        self.player1.feedReward(winner)\n",
    "                        self.player2.feedReward(-winner)\n",
    "                        results.append(winner)\n",
    "                        self.player1.reset()\n",
    "                        self.player2.reset()\n",
    "                        self.reset()\n",
    "                        break\n",
    "                    current_player = self.player2 if current_player == self.player1 else self.player1\n",
    "                else:\n",
    "                    current_player = self.player2 if current_player == self.player1 else self.player1\n",
    "                    if not (self.has_legal_moves(self.player1.color) or self.has_legal_moves(self.player2.color)):\n",
    "                        self.player1.feedReward(0)\n",
    "                        self.player2.feedReward(0)\n",
    "                        results.append(0)\n",
    "                        self.reset\n",
    "                        break\n",
    "            if i % inner_rounds==0:\n",
    "                self.player1.sVNNtrain()\n",
    "                self.player2.sVNNtrain()\n",
    "            self.reset()\n",
    "\n",
    "        return results\n",
    "    ## Test player\n",
    "    def play2(self):\n",
    "        self.reset()\n",
    "        current_player = self.player1 if self.current_play == 1 else self.player2\n",
    "        while True:\n",
    "            positions = self.has_legal_moves(current_player.color)\n",
    "            if positions:\n",
    "                p_move = current_player.choose_action(self)\n",
    "                self.execute_move(p_move, current_player.color)\n",
    "                if self.game_over():\n",
    "                    winner = self.get_reward()\n",
    "                    if winner == 1:\n",
    "                        return 1\n",
    "                    elif winner == -1:\n",
    "                        return -1\n",
    "                    else:\n",
    "                        return 0\n",
    "                    break\n",
    "                current_player = self.player2 if current_player == self.player1 else self.player1\n",
    "            else:\n",
    "                current_player = self.player2 if current_player == self.player1 else self.player1\n",
    "                if not (self.has_legal_moves(self.player1.color) or self.has_legal_moves(self.player2.color)):\n",
    "                    break\n",
    "        return 0\n",
    "\n",
    "    def NNPlay(self, rounds=200, inner_rounds=10,name_exp= None):\n",
    "\n",
    "\n",
    "        print(\"training...\")\n",
    "        w1=self.play(rounds)\n",
    "\n",
    "        self.player1.state_value_model.save(f\"model2_p1_{name_exp}.keras\")\n",
    "        self.player2.state_value_model.save(f\"model2_p2_{name_exp}.keras\")\n",
    "\n",
    "        analysis = []\n",
    "        train_batches = int(rounds/inner_rounds) + 1\n",
    "        for i in range(train_batches):\n",
    "            start = inner_rounds * i\n",
    "            end = inner_rounds * (i + 1)\n",
    "            temp_p = w1[start:end]\n",
    "            p1_wins = temp_p.count(1)\n",
    "            p2_wins = temp_p.count(-1)\n",
    "            ties = temp_p.count(0)\n",
    "            analysis.append([i, start, end, p1_wins, p2_wins, ties])\n",
    "\n",
    "        andf= pd.DataFrame(analysis)\n",
    "        andf.columns = ['batch', 'start', 'end', 'p1win', 'p2win', 'tie']\n",
    "        andf['p1win'] = andf['p1win'].apply(lambda x: x/inner_rounds)\n",
    "        andf['p2win'] = andf['p2win'].apply(lambda x: x/inner_rounds)\n",
    "        andf['tie'] = andf['tie'].apply(lambda x: x/inner_rounds)\n",
    "\n",
    "        # Plot\n",
    "        plt.plot(andf['batch'], andf['p1win'])\n",
    "        plt.plot(andf['batch'], andf['p2win'])\n",
    "        plt.plot(andf['batch'], andf['tie'])\n",
    "        plt.legend(['p1 win', 'p2 win', 'tie'])\n",
    "        #plt.title(title_string)\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    def print_board(self):\n",
    "        for i in range(self.n):\n",
    "            print('.........................')\n",
    "            print('|', end=' ')\n",
    "            for j in range(self.n):\n",
    "                if self.board[i][j] == 1:\n",
    "                    print('B', end=' ')\n",
    "                elif self.board[i][j] == -1:\n",
    "                    print('W', end=' ')\n",
    "                else:\n",
    "                    print(' ', end=' ')\n",
    "                print('|', end=' ')\n",
    "            print()\n",
    "        print('...........................')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M-Y9Ps7LppXC",
   "metadata": {
    "id": "M-Y9Ps7LppXC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "AdCws73wpqxc",
   "metadata": {
    "id": "AdCws73wpqxc"
   },
   "source": [
    "## DQN agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b986e968",
   "metadata": {
    "id": "b986e968"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, color, n=6, eps_decay=False, start_exp_rate=0.2, end_exp_rate=0.02):\n",
    "        self.n = n\n",
    "        self.color = color\n",
    "        self.states = []\n",
    "        self.lr = 1.0\n",
    "        self.exp_rate = start_exp_rate\n",
    "        self.decay_gamma = 0.9\n",
    "        self.states_value = {}\n",
    "        self.eps_decay = eps_decay\n",
    "        self.decay_rate= 0.005\n",
    "        self.start_exp_rate = start_exp_rate\n",
    "        self.end_exp_rate = end_exp_rate\n",
    "        self.state_value_model = self.neural_network()\n",
    "\n",
    "    def neural_network(self):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(Dense(units=36, input_dim=self.n * self.n, activation='relu'))\n",
    "        model.add(Dense(18, activation='relu'))\n",
    "        model.add(Dense(9, activation='relu'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    def get_hash(self, board):\n",
    "        state_old = board.get_board_state()\n",
    "        return np.array(state_old).flatten()\n",
    "\n",
    "    def sNvalue(self, board):\n",
    "        state_val = self.get_hash(board)\n",
    "        state_val = state_val.reshape((1, self.n * self.n))\n",
    "        pred = self.state_value_model.predict(state_val)[0]\n",
    "        return pred\n",
    "\n",
    "    def savemodel(self,path):\n",
    "        self.state_value_model.save(path)\n",
    "\n",
    "    def loadSVNNmodel(self, path):\n",
    "\n",
    "        self.state_value_model = keras.models.load_model(path)\n",
    "\n",
    "    ## Exponential epsilon decay\n",
    "    def exponential_decay_value(self, current_step):\n",
    "        epsilon = self.end_exp_rate + (self.start_exp_rate - self.end_exp_rate) * np.exp(-self.decay_rate * current_step)\n",
    "        return epsilon\n",
    "\n",
    "    def setEps(self, total_games, current_step):\n",
    "        if self.eps_decay==False:\n",
    "            return\n",
    "        else:\n",
    "            self.exp_rate = self.exponential_decay_value(current_step)\n",
    "            if current_step % 10 == 0:\n",
    "                print('Exploration rate modified at game {} with current value of {}'.format(current_step, self.exp_rate))\n",
    "\n",
    "\n",
    "    def choose_action(self, board):\n",
    "        legal_moves = board.get_legal_moves(self.color)\n",
    "        if np.random.uniform(0,1) < self.exp_rate:\n",
    "            action_index = np.random.choice(len(legal_moves))\n",
    "            selected_action = legal_moves[action_index]\n",
    "            return selected_action\n",
    "        else:\n",
    "            value_max = -999\n",
    "            best_action = None\n",
    "            for p in legal_moves:\n",
    "                next_board = copy.deepcopy(board)\n",
    "                next_board.execute_move(p, self.color)\n",
    "                next_state_hash = self.get_hash(next_board)\n",
    "                pred = self.sNvalue(next_board)\n",
    "                if pred >= value_max:\n",
    "                    value_max = pred\n",
    "                    best_action = p\n",
    "            return best_action\n",
    "\n",
    "    def getbuffer(self):\n",
    "        data = self.states_value\n",
    "        ll = []\n",
    "        for k in data.keys():\n",
    "            k_str = str(k)\n",
    "            yy = re.findall(r'[-/+]?\\d+\\.*\\d*', k_str)\n",
    "            zz = data.get(k)\n",
    "            yy.append(zz)\n",
    "            ll.append(yy)\n",
    "        lldf = pd.DataFrame(ll)\n",
    "        cols = ['x'+str(i) for i in range(self.n*self.n)]\n",
    "        cols.append('val')\n",
    "        lldf.columns = cols\n",
    "        lldf.to_csv('Buffer1.csv', index=False)\n",
    "        return\n",
    "\n",
    "    def sVNNtrain(self, Xin=None, Yin=None):\n",
    "        self.getbuffer()\n",
    "        df = pd.read_csv('Buffer1.csv')\n",
    "        print ('length of buffer is', len(df))\n",
    "        traincols=['x' + str(i) for i in range(self.n*self.n)]\n",
    "        testcol = 'val'\n",
    "        if Xin is None:\n",
    "            Xin = df[traincols]\n",
    "\n",
    "            Yin = df[testcol]\n",
    "        self.state_value_model.fit(Xin, Yin, epochs=10, verbose=False)\n",
    "        self.states_value = {}\n",
    "        return\n",
    "    def addState(self, state):\n",
    "        state_tuple = tuple(map(tuple, state))\n",
    "        self.states.append(state_tuple)\n",
    "\n",
    "    def feedReward(self, reward):\n",
    "        for st in reversed(self.states):\n",
    "            if self.states_value.get(st) is None:\n",
    "                self.states_value[st] = 0\n",
    "            self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])\n",
    "            reward = self.states_value[st]\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "V2UIz44rpgoc",
   "metadata": {
    "id": "V2UIz44rpgoc"
   },
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6zvsuVL5pj1G",
   "metadata": {
    "id": "6zvsuVL5pj1G"
   },
   "source": [
    "## Random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e32b051b",
   "metadata": {
    "id": "e32b051b"
   },
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def __init__(self, color, name):\n",
    "        self.color = color\n",
    "        self.name = name\n",
    "\n",
    "    def choose_action(self, board):\n",
    "        legal_moves = board.get_legal_moves(self.color)\n",
    "        if legal_moves:\n",
    "            return random.choice(legal_moves)\n",
    "        return None\n",
    "\n",
    "    def addState(self, state, action, reward, next_state, done):\n",
    "        pass\n",
    "\n",
    "    def feedReward(self, minibatch):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe8622",
   "metadata": {
    "id": "d1fe8622"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cd671e8",
   "metadata": {
    "id": "4cd671e8"
   },
   "outputs": [],
   "source": [
    "# Test Method :\n",
    "\n",
    "def testgame(player1, player2, ngames):\n",
    "    p1_wins = 0\n",
    "    p2_wins = 0\n",
    "    ties = 0\n",
    "\n",
    "    for i in range(1, ngames + 1):\n",
    "        board = Board(player1, player2, n=6)\n",
    "        win = board.play2()\n",
    "        if win == 1:\n",
    "            p1_wins += 1\n",
    "        elif win == -1:\n",
    "            p2_wins += 1\n",
    "        else:\n",
    "            ties += 1\n",
    "\n",
    "        # if i % 10 == 0:\n",
    "        #     print(f\"Games played: {i}\")\n",
    "\n",
    "    p1_win_probability = p1_wins / ngames\n",
    "    p2_win_probability = p2_wins / ngames\n",
    "    tie_probability = ties / ngames\n",
    "    return p1_win_probability, p2_win_probability, tie_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a353e56",
   "metadata": {},
   "source": [
    "## Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355c956",
   "metadata": {},
   "source": [
    "## First player as Random and Second player as Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5gJRA-CrNBZ9",
   "metadata": {
    "id": "5gJRA-CrNBZ9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\tumpa_anaconda\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Player 1 (1): Winning Probability: 23.00%\n",
      "Player 2 (-1): Winning Probability: 76.00%\n",
      "Tie Probability: 1.00%\n"
     ]
    }
   ],
   "source": [
    "# # Test 3 - both player trained with decaying epsilon\n",
    "\n",
    "player1 = DQNAgent(1,n=6, start_exp_rate=2)\n",
    "player2 = DQNAgent(-1,n=6,start_exp_rate=0)\n",
    "player2.loadSVNNmodel(\"model2_p2_DP1_DP2.keras\")\n",
    "\n",
    "p1_win_probability,p2_win_probability,tie_probability=testgame(player1,player2, 100)\n",
    "print(\"Player 1 ({}): Winning Probability: {:.2%}\".format(player1.color, p1_win_probability))\n",
    "print(\"Player 2 ({}): Winning Probability: {:.2%}\".format(player2.color, p2_win_probability))\n",
    "print(\"Tie Probability: {:.2%}\".format(tie_probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae35d12",
   "metadata": {},
   "source": [
    "## First player as Computer and Second player as Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "XSttiHuZKGpt",
   "metadata": {
    "id": "XSttiHuZKGpt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 (1): Winning Probability: 72.00%\n",
      "Player 2 (-1): Winning Probability: 23.00%\n",
      "Tie Probability: 5.00%\n"
     ]
    }
   ],
   "source": [
    "# # Test 3 - both player trained with decaying epsilon\n",
    "player1 = DQNAgent(1,n=6, start_exp_rate=0)\n",
    "player1.loadSVNNmodel('model2_p1_DP1_DP2.keras')\n",
    "player2 = DQNAgent(-1,n=6,start_exp_rate=2)\n",
    "\n",
    "\n",
    "p1_win_probability,p2_win_probability,tie_probability=testgame(player1,player2, 100)\n",
    "print(\"Player 1 ({}): Winning Probability: {:.2%}\".format(player1.color, p1_win_probability))\n",
    "print(\"Player 2 ({}): Winning Probability: {:.2%}\".format(player2.color, p2_win_probability))\n",
    "print(\"Tie Probability: {:.2%}\".format(tie_probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97466e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0023677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
